The Big Bang is actually not a "theory" at all, but rather a scenario or model about the early moments of our universe, for which the evidence is overwhelming. It is a common misconception that the Big Bang was the origin of the universe. In reality, the Big Bang scenario is completely silent about how the universe came into existence in the first place. In fact, the closer we look to time "zero," the less certain we are about what actually happened, because our current description of physical laws do not yet apply to such extremes of nature. The Big Bang scenario simply assumes that space, time, and energy already existed. But it tells us nothing about where they came from or why the universe was born hot and dense to begin with. But if space and everything with it is expanding now, then the universe must have been much denser in the past. That is, all the matter and energy (such as light) that we observe in the universe would have been compressed into a much smaller space in the past. Einstein's theory of gravity enables us to run the "movie" of the universe backwards—i.e., to calculate the density that the universe must have had in the past. The result: any chunk of the universe we can observe—no matter how large—must have expanded from an infinitesimally small volume of space. By determining how fast the universe is expanding now, and then "running the movie of the universe" backwards in time, we can determine the age of the universe. The result is that space started expanding 13.77 billion years ago. This number has now been experimentally determined to within 1% accuracy. It's a common misconception that the entire universe began from a point. If the whole universe is infinitely large today (and we don't know yet), then it would have been infinitely large in the past, including during the Big Bang. But any finite chunk of the universe—such as the part of the universe we can observe today—is predicted to have started from an extremely small volume. Part of the confusion is that scientists sometimes use the term "universe" when they're referring to just the part we can see "the observable universe". And sometimes they use the term universe to refer to everything, including the part of the universe beyond what we can see. It's also a common misconception that the Big Bang was an "explosion" that took place somewhere in space. But the Big Bang was an expansion of space itself. Every part of space participated in it. For example, the part of space occupied by the Earth, the Sun, and our Milky Way galaxy was once, during the Big Bang, incredibly hot and dense. The same holds true of every other part of the universe we can see. We observe that galaxies are rushing apart in just the way predicted by the Big Bang model. But there are other important observations that support the Big Bang. Astronomers have detected, throughout the universe, two chemical elements that could only have been created during the Big Bang: hydrogen and helium. Furthermore, these elements are observed in just the proportions (roughly 75% hydrogen, 25% helium) predicted to have been produced during the Big Bang. This is the nucleosynthesis of the light elements. This prediction is based on our well-established understanding of nuclear reactions— independent of Einstein's theory of gravity. Second, we can actually detect the light left over from the era of the Big Bang. This is the origin of the cosmic microwave background radiation. The blinding light that was present in our region of space has long since travelled off to the far reaches of the universe. But light from distant parts of the universe is just now arriving here at Earth, billions of years after the Big Bang. This light is observed to have all the characteristics expected from the Big Bang scenario and from our understanding of heat and light. The standard Hot Big Bang model also provides a framework in which to understand the collapse of matter to form galaxies and other large-scale structures observed in the Universe today. At about 10,000 years after the Big Bang, the temperature had fallen to such an extent that the energy density of the Universe began to be dominated by massive particles, rather than the light and other radiation which had predominated earlier. This change in the form of matter density meant that the gravitational forces between the massive particles could begin to take effect, so that any small perturbations in their density would grow. Thirteen point eight billion years later we see the results of this collapse in the structure and distribution of the galaxies. The best estimate of the age of the universe as of 2013 is 13.798 ± 0.037 billion years but due to the expansion of space humans are observing objects that were originally much closer but are now considerably farther away (as defined in terms of cosmological proper distance, which is equal to the co-moving distance at the present time) than a static 13.8 billion light-years distance. The diameter of the observable universe is estimated at about 93 billion light-years (28 billion parsecs), putting the edge of the observable universe at about 46–47 billion light-years away. The co-moving distance from Earth to the edge of the "visible" universe (also called the particle horizon) is about 46.5 billion light-years in any direction. This defines a lower limit on the co-moving radius of the "observable" universe, although it is expected that the visible universe is somewhat smaller than the observable universe since we see only light from the cosmic microwave background radiation that was emitted after the time of recombination, giving us the spherical surface of last scattering. The visible universe is thus a sphere with a diameter of about 93 billion light-years. The Hubble Deep Field visible-light (HDF), released in 1996, looked back to within 1.0 billion years after the Big Bang. The Hubble Ultra Deep Field visible-light (HUDF), released March 2004, looks back even further to a time only 0.7 billion years after the Big Bang, close to the period when the first galaxies formed. This Hubble Ultra Deep Field (HUDF) view of nearly 10,000 galaxies was the deepest visible-light image of the cosmos in 2004. This galaxy-studded view of the Hubble Ultra Deep Field represents a "deep" core sample of the universe, cutting across billions of light-years. HUDF is an image of a small region of space in the constellation Fornax, composited from Hubble Space Telescope data accumulated over a period from September 3, 2003 through January 16, 2004. The patch of sky in which the galaxies reside was chosen because it had a low density of bright stars in the near-field. In vibrant contrast to the rich harvest of classic spiral and elliptical galaxies, there is also a zoo of oddball galaxies littering the field, as shown in this close-up view of the HUDF. Some look like toothpicks; others like links on a bracelet. A few appear to be interacting. These oddball galaxies chronicle a period when the universe was younger and more chaotic. Order and structure were just beginning to emerge. When did galaxies form? To find out, the deepest near-infrared image of the sky ever, has been taken of the same field as the optical-light Hubble Ultra Deep Field (HUDF) in 2004. This image was taken the summer of 2009, by the newly installed Wide Field Camera 3 on the refurbished Hubble Space Telescope. Faint red smudges identified on this image likely surpass redshift 8 in distance. These galaxies, therefore, likely existed when the universe was only a few percent of its present age, and may well be members of the first class of galaxies. This early class of low luminosity galaxies likely contained energetic stars emitting light that transformed much of the remaining normal matter in the universe from a cold gas to a hot ionized plasma. Some large modern galaxies make a colorful foreground to these distant galaxies. The Hubble eXtreme Deep Field (XDF) is an image of a small part of space in the center of the Hubble Ultra Deep Field within the constellation Fornax, showing the deepest optical view in space. Released on September 25, 2012, the XDF image compiled 10 years of previous images and shows galaxies from 13.2 billion years ago. The exposure time was two million seconds, or approximately 23 days. The faintest galaxies are one ten-billionth the brightness of what the human eye can see. Many of the smaller galaxies are very young galaxies that eventually became the major galaxies, like the Milky Way and other galaxies in our galactic neighbourhood. This illustration separates the XDF into three planes showing foreground, background, and very far background galaxies. These divisions reflect different epochs in the evolving universe. Fully mature galaxies are in the foreground plane that shows galaxies as they looked less than 5 billion years ago. The universe is rich in evolving, nearly mature galaxies from 5 to 9 billion years ago. Beyond 9 billion years the universe is awash in compact galaxies and proto-galaxies, blazing with young stars. Stellar 'Fireworks Finale' Came First in the Young Universe—subsequent analysis of Hubble Space Telescope deep sky images supported the theory that the first stars in the universe appeared in an abrupt eruption of star formation, rather than at a gradual pace. The universe could go on making stars for trillions of years to come, before all the hydrogen is used up, or is too diffuse to coalesce. But the universe will never again resemble the star-studded tapestry that brought light to the darkness. This is an artist's impression of how the very early universe might have looked when it went through a voracious onset of star formation, converting primordial hydrogen into myriad stars at an unprecedented rate. Back then the sky would have looked markedly different from the sea of quiescent galaxies around us today. This sky is ablaze with primeval starburst galaxies; giant elliptical and spiral galaxies have yet to form. Within the starburst galaxies, bright knots of hot blue stars come and go like bursting fireworks shells. The most massive stars self-detonate as supernovas, which explode across the sky like a string of firecrackers. The foreground starburst galaxies at the lower right are sculpted with hot bubbles from supernova explosions and torrential stellar winds. Although no stars and galaxies existed just after the Big Bang, the young cosmos was anything but dull. It was humming with activity. In the beginning, physical conditions were so extreme that matter as we know it today did not exist. During the early part of its existence, after one times ten to the minus 12th of a second, our universe was so small and dense that light and matter intertwined; space was hot, dark, and ionized—filled with a plasma of charged particles. By the time the universe was one second old, the temperatures and densities had dropped enough for protons and neutrons to form from quarks. Within the next few minutes, the nuclei of the light elements, hydrogen, helium, and lithium, were created in a process called primal or Big Bang nucleosynthesis. The universe at this point was cooling rapidly enough to shut off the process of nucleosynthesis before elements heavier than boron could form. About four hundred thousand years after the Big Bang the cosmos had grown large enough for matter and energy to move through space without immediately colliding—ending the plasma state of the early universe. The universe had cooled to about 3,000 degrees Celsius (5,400 degrees Fahrenheit) allowing electrons, protons, and neutrons to come together to form neutral atoms—the basic building blocks of all visible matter in the universe. This marked the “Decoupling” of matter and energy that we detect today as the cosmic microwave background radiation. This radiation has been stretched and cooled by the expansion of the universe from three thousand degrees to minus 270.42 degrees Celsius, or just three degrees above absolute zero. At this point the universe was made up mostly of clouds of hydrogen and helium atoms. As the universe expanded and cooled, some regions of space amassed slightly higher densities of hydrogen. As millions of years passed, the slight differences grew large, as dense areas drew in material because they had more gravity. Researchers have dubbed this period of coalescing the "Dark Ages."
The Cosmic Background Explorer (COBE) satellite was launched in 1989, twenty five years after the discovery of the microwave background radiation in 1964. In 1992, the COBE team announced that they had discovered “ripples at the edge of the universe”, that is, the first sign of primordial fluctuations at 380,000 years after the Big Bang. These are the imprint of the seeds of galaxy formation. These appear as temperature variations on the full sky map that COBE obtained (shown above). Red areas represent areas with slightly higher temperatures and blue areas a slightly lower temperature than the mean. In 2006, two American astronomers, John C. Mather of the NASA Goddard Space Flight Center in Greenbelt, Md., and George F. Smoot of the Lawrence Berkeley National Laboratory at the University of California, Berkeley, won the Nobel Prize in Physics for their work on the COBE project. The WMAP mission was proposed to NASA in 1995 and launched in 2001. The final command to stop collecting data was transmitted to the WMAP satellite on August 19th 2010. Analyses of a high-resolution map released in 2003, of microwave light emitted only 380,000 years after the Big Bang (pictured above) appear to define our universe more precisely than ever before. The results from the orbiting Wilkinson Microwave Anisotropy Probe resolve several long-standing disagreements in cosmology rooted in less precise data. Specifically, present analyses of the WMAP all-sky map indicate that the universe is 13.7 billion years old (accurate to 1 percent), composed of 74 percent "dark energy", 22 percent cold "dark matter", and only 4 percent atoms, is currently expanding at the rate of 71 km/sec/Mpc (accurate to 5 percent), and underwent an episode of rapid expansion called "inflation". The universe is 13.73 billion years old, give or take 120 million years, astronomers announced in early March 2008. That age, based on precision measurements of the oldest light in the universe, agrees with results announced in 2006. Two additional years of data from a NASA's Wilkinson Microwave Anisotropy Probe have narrowed the uncertainty by tens of millions of years (Chang, 2008). About 380,000 years after the Big Bang, the universe cooled enough for protons and electrons to combine into hydrogen atoms. That released a burst of light, which over the billions of years since has cooled to a bath of microwaves pervading the cosmos. Yet there are slight variations in the background, which the NASA satellite had been measuring since 2001. Those variations have given evidence supporting an idea known as cosmic inflation, a rapid expansion of the universe in the first trillionth of a trillionth of a second of its existence. The new set of data was precise enough to differentiate between various proposed models of inflation. Astronomers can also now see strong evidence for the universe being awash in almost mass-less subatomic particles known as neutrinos. This sea of primordial neutrinos created in the Big Bang was expected. The new data also refined findings that the earliest stars switched on 400 million years after the Big Bang. The starlight started breaking up interstellar hydrogen atoms back into charged protons and electrons—creating a fog that deflected the cosmic microwaves—but took half a billion years to break apart all of the atoms. This image allows a comparison of the resolution of the Holmdel Horn Antenna in 1965, to COBE in 1992, to WMAP in 2003, a time span of 38 years. The vague light across the horn antenna view of the CMB and the red streaks across COBE and WMAP are light from the Milky Way. The expansion of the universe over most of its history has been relatively gradual. The notion that a rapid period "inflation" preceded the Big Bang expansion was first put forth 25 years ago by Alan Guth. The new WMAP observations favor specific inflation scenarios over other long held ideas. WMAP data from 2008 revealed that the universe's contents include ~ 4.6% atoms, the building blocks of stars and planets. Dark matter comprised ~ 23% of the universe. This matter, different from atoms, does not emit or absorb light. It has only been detected indirectly by its gravity. And that ~ 72% of the universe is composed of "dark energy" that acts as a sort of anti-gravity. This energy, distinct from dark matter, is responsible for the present-day acceleration of the universal expansion. WMAP data is accurate to two digits, so the total of these numbers is not 100%. This reflects the limit of WMAP's ability to define Dark Matter and Dark Energy. "Most of us think of the universe as all the matter there is, and by matter we mean the stuff we can see from afar or could touch if it were up close. But the motion of the observable objects in the universe, like stars and galaxies and clouds of gas, make no sense if the universe contains only ordinary, perceptible matter. This became apparent in 1933, thanks to an astronomer named Fritz Zwicky. He discovered that parts of a distant cluster of galaxies were moving too fast to remain within the cluster if it contained only ordinary matter. He concluded that "dark matter", a phrase he coined, held the cluster together. But for 70-plus years since, no one had observed dark matter. It would be like seeing gravity. That has now changed." Astronomers using ground-based telescopes and satellite observatories have witnessed a separation between visible matter and the dark matter that shapes its motions (see above). It occurred 100 million years ago when two galaxy clusters three billion light-years away passed through each other at about 10 million miles an hour. Imagine two crowds of pedestrians on a collision course. Some people in both groups— no doubt dressed in black—basically refuse to engage with anyone and just keep moving. But the ordinary people want to stop and chat. As the two crowds merge and then head in opposite directions, the people in black will have pushed ahead, separating themselves from the rest. That, in a nutshell, is what the astronomers saw, minus the people, of course. Observing what was predicted a lifetime ago is an extraordinary accomplishment. It confirms that this part of our picture of the universe is essentially correct. But observing dark matter and knowing what it is are very different, and we are nowhere near the latter. The matter in galaxy cluster 1E 0657-56, known as the "bullet cluster", is shown in the composite image on the previous page. The bullet cluster's individual galaxies are seen in the optical image data, but their total mass adds up to far less than the mass of the cluster's two clouds of hot x-ray emitting gas shown in red. Representing even more mass than the optical galaxies and x-ray gas combined, the blue hues show the distribution of dark matter in the cluster. Otherwise invisible to telescopic views, the dark matter was mapped by observations of gravitational lensing of background galaxies. In a text book example of a shock front, the bullet-shaped cloud of gas at the right was distorted during the titanic collision between two galaxy clusters that created the larger bullet cluster itself. But the dark matter present has not interacted with the cluster gas except by gravity. The clear separation of dark matter and gas clouds is considered direct evidence that dark matter exists. Dr. Allan Sandage, the Carnegie Observatories astronomer, once called cosmology "the search for two numbers" The first number is the Hubble constant, which tells how fast the universe is expanding. Together with the other number telling how fast the expansion is slowing, they determine whether the universe will expand forever or not. The second number, known as the deceleration parameter, indicates how much the cosmos had been warped by the density of its contents. In a high-density universe, space would be curved around on itself like a ball. Such a universe would eventually stop expanding and fall back together in a big crunch that would extinguish space and time, as well as the galaxies and stars that inhabit them. A low-density universe, on the other hand, would have an opposite or "open" curvature like a saddle, harder to envision, and would expand forever. In between with no overall warpage at all was a "Goldilocks" universe with just the right density to expand forever but more and more slowly, so that after an infinite time it would coast to a stop. This was a "flat" universe in the cosmological parlance, and to many theorists the simplest and most mathematically beautiful solution of all. "Beginning in 1998, the cozy picture of a flat, ever expanding universe began to unravel. In 1998, two research groups, working independently, one led by Saul Perimutter, the other by Brian Schmidt, both made the same startling discovery. Over the past five billion years the expansion of the universe has been speeding up, not slowing down as it would under the influence of gravity alone. Since then the evidence for a cosmic speedup has gotten much stronger and has revealed not only a current accelerating phase but an earlier epoch of deceleration dominated by gravity. Added to the question of what is causing the acceleration, a flat universe requires a critical energy density, but ordinary matter even combined with cold dark matter together comprise only 26 present of the needed mass, leaving the balance of 74 percent to be in the form of a mysterious "dark energy". "One proposal for what is driving the current accelerating phase of the universe is the energy of space itself. In quantum mechanics even empty space has an energy density in the form of virtual particles that appear and then disappear almost instantaneously. On the very small scales where quantum effects become important, even empty space is not really empty. Instead virtual particle-antiparticle pairs pop out of the vacuum travel for a short distance and then disappear again on timescales so fleeting that one cannot observe them directly. Yet their indirect effects are very important and can be measured. This vacuum energy is now thought of as Einstein's cosmological term. This new concept of the cosmological term, however, is quite different from the one Einstein introduced into his equations. The problem with this picture, however, is that all calculations and estimates of the magnitude of the empty-space energy so far, lead to absurdly large values. It is also possible that the explanation of cosmic acceleration will have nothing to do with resolving the mystery of why the cosmological term is so small or how Einstein's theory can be extended to include quantum mechanics. General relativity stipulates that an object's gravity is proportional to its energy density plus three times its internal pressure. Any energy form with a large, negative pressure—which pulls inward like a rubber sheet instead of pushing outward like a ball of gas—will therefore have repulsive gravity. So cosmic acceleration may simply have revealed the existence of an unusual energy form, dubbed "dark energy", that is not predicted by either quantum mechanics or string theory." "No one knows how the first space, time, and matter arose. And scientists are grappling with even deeper questions. If there was nothing to begin with, then where did the laws of nature come from? How did the universe "know" how to proceed? And why do the laws of nature produce a universe that is so hospitable to life? As difficult as these questions are, scientists are attempting to address them with bold new ideas—and new experiments to test those ideas. Understanding how the universe began requires developing a better theory of how space, time, and matter are related. In physics, a theory is not a guess or a hypothesis. It is a mathematical model that lets us make predictions about how the world behaves. Einstein's theory of gravity, for example, accurately describes how matter responds to gravity in the large-scale world around us. And our best theory of the tiny sub-atomic realm, called quantum theory, makes very accurate predictions about the behavior of matter at tiny scales of distance. But these two theories are not complete and are not able to make accurate predictions about the very earliest moments when the universe was both extremely dense and extremely small. Some of the best minds in physics are working on a new theory of space, time, and matter, called "string theory," that may help us better understand where the universe came from.
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √ dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of √ 1 dk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by √ 1 dk . Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi , zi ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.
In the early modern period, due to colonialism and empire building, European naturalists, working in centralized botanical gardens and national zoos, investigated an unprecedented variety of animal and plant specimens. Starting in the 18th century, naturalists began to systematically investigate the fossil remains of various organisms and compare these with living organisms. In the early half of the 19th century, it became clear that there had once existed entire families of flora and fauna (plants and animals) that had passed out of existence, and that moreover, in the periods – that is, geological strata – in which these creatures existed, much of the flora and fauna that are alive today did not exist. The evidence for large-scale biological change was gathered slowly and was still ongoing when Darwin was working. Although there was a lot of disagreement about how these changes had taken place, and what they implied, by Darwin’s time, most naturalists accepted that there had been some changes in biological species. However, even if we accept that there has been change in species throughout the history of the earth, we might have several different theories about how this change occurred. All of the theories advanced before Darwin argued for some kind of directed change – in some sense responding to, and hence directly influenced by, the environment and the actions of organisms. Darwin tried to distinguish his theories from these by arguing that evolutionary changes were based only on naturally occurring processes – processes that are still occurring around us now. The theory of evolution by natural selection is a theory about the mechanism by which evolution occurred in the past, and is still occurring now. The basic theory was developed by both Darwin and Wallace, however, Darwin gave a much fuller argument. The theory states that biological change takes place with two basic characteristics: 1) Variation: Random variations occur in the traits of individual organisms and are passed on to their offspring. 2) Struggle for existence: There is an existential competition that insures advantageous traits are preserved and disadvantageous traits are eliminated. A key result of evolution by natural selection is that it forces us to look at various binaries such as advantage and disadvantage, superior and inferior from a local perspective. This means that there cannot be traits that are better in any absolute sense – there can only be traits that are better suited for a particular set of circumstances. In an anoxic environment, being able to consume sulphates is more useful than, say, general intelligence. Within a species, individuals specialize at exploiting the environment in various ways, and over long periods this specialization creates divergence. A group of birds with longer beaks might prefer worms, while a group with stronger beaks might prefer nuts. Being attractive to, and interacting with, mates of the opposite sex is an exploitation of local circumstances. One of the most controversial aspects of the theory of evolution is the implications that it has for human beings. According to current thinking, there were a number of other species (or varieties) of the homo genus that were closely related to humans, but which are now all extinct (Homo habilis, H. erectus, H. ergaster, H. heidelbergensis, H. neanderthalensis, H. floresiensis, etc.). H. habilis lived some 2.3 million years ago, whereas the Neanderthals died out about 24,000 years ago. The whole genus is thought to be about 2.5 million years old and to have diverged from the pan genus of great apes about 5 million years ago. In the 19th century, there were many conflicting ideas about evolution and the meaning of the existence of variations among organisms. Many natural philosophers, believed that God had created a fixed number of species and that since that time there had been hybridization and mixing (variation), but no fundamental change. In the 1790s, Darwin’s own grandfather, Erasmus, had published poems putting forward the idea of evolution. In the 1810s, Jean-Baptiste Lamarck outlined a comprehensive theory of evolution in which animals, individually, become more complex and better adapted and then pass these changes onto their offspring. This kind of theory is known as Lamarkism. In 1844, Vestiges of the Natural History of Creation (Robert Chambers) presented a natural history in which all forms were in transformation and everything develops from previous forms. In the 18th and 19th centuries the empirical study of rocks and minerals was of increasing economic importance. The earliest theories of rock formations were developed in the mining schools in the German states. Abraham Werner (1749–1817), a professor at the mining school in Freiburg, set out a theory of stratification, in which he claimed that different types of rocks were laid down in different periods of the earth’s history by the gradual drying of a primordial ocean. In the early 19th century, this was combined with a hypothesis of cataclysmic changes and developed as a theory that helped geologists prospect for different types of rocks and minerals. There were also conservative thinkers, who linked the primordial ocean of neptunism with Noah’s flood mentioned in the Bible, but this was not the majority. In the 19th century, it became clear that the same kinds of rocks had been formed in different periods of the earth’s history, so geologists began to study the fossils embedded in the strata. It became clear that there were the same kinds of fossils at comparable levels all over the earth. Geologist like James Hutton (1726–1797) and Charles Lyell (1797–1875) began to focus on the role of volcanoes and argued that the earth had a molten core – which was called plutonism. They argued that geological formations are not the result of cataclysmic changes in the past, but the long-term gradual result of processes we see around us now. This became known as uniformitarianism. Beginning in the end of the 18th century, the rising industrialism in Britain created a massive growth of urban poor. This lead to the rise of poverty as a social problem and to various attempts to address it. In 1798, Thomas Malthus (1766–1834) published his Essay on the Principle of Population. Malthus argued that human population increases geometrically, while food supplies increase arithmetically. This means that at some point, population growth will always outstrip food supplies and there will be a struggle for existence, especially among the poor. Thus, Mathus saw the conditions of poverty, which had been created by the Industrial Revolution, as a necessary fact of the human condition. This was a thoroughly pessimistic doctrine. After returning from the voyage of the Beagle, Darwin began to publish his ideas about the geography of South America, handbooks of the flora and fauna of the places he visited and an account of his travels. He made a name for himself as a natualist and became a friend and colleague of many of the most important biologists in England. Occasionally, he would try to interest someone in his ideas about transmutation, but no one was convinced. Because Darwin was himself a member of the social elite, he tried to distance himself from other evolutionists, such as Lamarck and Chambers. Also, his wife was very religious. At this time, in Britain, evolution was regarded as a dangerous idea, associated with France, the revolution, and social and political unrest. When Darwin realized that the established scientists were not interested in his ideas, he began to cultivate the friendship of younger men who were on the rise such as Thomas Huxley (1825–1895) and Joseph Hooker (1817–1911). Over a period of many years, through correspondence, he tried to convince them of his ideas. In the 1850s, he also started a correspondence with Alfred Wallace (1823–1913). This lead to Wallace sending Darwin a short paper in which he independently described a theory of evolution by natural selection. It was decided by Lyell and Hooker, who both knew of Darwin’s work, that the two men should make a joint presentation at the Linnean Society acknowledging their independent discovery. No one took any notice of this. Darwin then spent over a year developing the argument which was published as On the Origin of Species, 1859. Darwin pointed out that animal breeders artificially select preferred characteristics from domestic populations (pigeons, cows, dogs, etc.) in which there is a lot of random variation. Over many generation these small differences produce very different types of animals (varieties of the same species). Darwin argued that a similar selection occurs in nature. He then began to discuss the nature of species. He pointed out that it is often hard to demarcate between species; it is hard to decide what are different species as opposed to different varieties. He argued that we could think of well-marked varieties as “incipient species” – that is, newly forming, or emerging. : Darwin argued that individual organisms and species are all in a struggle for existence. This was contrary to the prevailing view – that nature was harmonious and showed evidence of a beneficent creator – a doctrine known as natural theology. Instead, Darwin claimed that life is a complex struggle – not always dramatically violent, as in predators and prey, but usually more subtle, as a plant struggling against drought, or the introduction of new species into a habitat, etc. He argued that this struggle then resulted in what he called “natural selection.” In the 6th edition of the book, he adopted the terminology of Herbert Spencer and referred to the “survival of the fittest.” “A struggle for existence inevitably follows from the high rate at which all organic beings tend to increase. Every being…must suffer destruction during some period of its life…otherwise, on the principle of geometrical increase, its numbers would quickly become so inordinately great that no country could support the product. Hence, as more individuals are produced than can possibly survive, there must in every case be a struggle for existence, either one individual with another of the same species, or with the individuals of distinct species, or with the physical conditions of life. It is the doctrine of Malthus applied with manifold force to the whole animal and vegetable kingdoms; for in this case there can be no artificial increase of food…” : In this chapter, Darwin explains the core of his theory: Nature varies, randomly. Some of variations are more advantageous than others. Because there is a struggle for existence, any advantage will be important, however slight. In the long run, those organisms that have a slight innate advantage will survive more often than those that do not, passing on their advantages to their offspring. Through this mechanism, Darwin argued that a population will change by the accumulation of small, but favorable advantages, over vast periods of time. Again, he referred to artificial selection, calling natural selection “unconscious artificial selection.” But notice that nature does not chose preferred traits in the way that a breeder does. “Owing to this struggle [for existence], variations, however slight and from whatever cause proceeding, if they be in any degree profitable to the individuals of a species, in their infinitely complex relations to other organic beings and to their physical conditions of life, will tend to the preservation of such individuals, and will generally be inherited by the offspring. The offspring, also, will thus have a better chance of surviving, for, of the many individuals of any species which are periodically born, but a small number can survive. I have called this principle, by which each slight variation, if useful, is preserved, by the term Natural Selection, in order to mark its relation to man’s power of selection. But the expression often used by Mr. Herbert Spencer of the Survival of the Fittest is more accurate, and is sometimes equally convenient.” The Origin was a very popular book, and went through six editions from 1859 to 1872. Darwin tapped into the thinking of the time and pitched evolution as a form of progress similar to the progress of the English nation that his contemporaries perceived. England was no longer as conservative as it had been when he was young and a belief in social progress using science and industry fit well with the evolutionary idea of biological progress. Literal interpretations of the Bible were coming under increasing criticism and many were prepared to be convinced of the fact of biological evolution. Darwin’s close supporters, such as Huxley, Hooker and Lyell, were influential and helped Darwin carry out the social aspect of the argument for evolution.
It is not difficult to understand why, in spite of this, we feel constrained to call the propositions of geometry “true.” Geometrical ideas correspond to more or less exact objects in nature, and these last are undoubtedly the exclusive cause of the genesis of those ideas. Geometry ought to refrain from such a course, in order to give to its structure the largest possible logical unity. The practice, for example, of seeing in a “distance” two marked positions on a practically rigid body is something which is lodged deeply in our habit of thought. We are accustomed further to regard three points as being situated on a straight line, if their apparent positions can be made to coincide for observation with one eye, under suitable choice of our place of observation. If, in pursuance of our habit of thought, we now supplement the propositions of Euclidean geometry by the single proposition that two points on a practically rigid body always correspond to the same distance (line-interval), independently of any changes in position to which we may subject the body, the propositions of Euclidean geometry then resolve themselves into propositions on the possible relative position of practically rigid bodies. Geometry which has been supplemented in this way is then to be treated as a branch of physics. We can now legitimately ask as to the “truth” of geometrical propositions interpreted in this way, since we are justified in asking whether these propositions are satisfied for those real things we have associated with the geometrical ideas. In less exact terms we can express this by saying that by the “truth” of a geometrical proposition in this sense we understand its validity for a construction with ruler and compasses. Of course the conviction of the “truth” of geometrical propositions in this sense is founded exclusively on rather incomplete experience. For the present we shall assume the “truth” of the geometrical propositions, then at a later stage (in the general theory of relativity) we shall see that this “truth” is limited, and we shall consider the extent of its limitation. On the basis of the physical interpretation of distance which has been indicated, we are also in a position to establish the distance between two points on a rigid body by means of measurements. For this purpose we require a “distance” (rod S) which is to be used once and for all, and which we employ as a standard measure. If, now, A and B are two points on a rigid body, we can construct the line joining them according to the rules of geometry; then, starting from A, we can mark off the distance S time after time until we reach B. The number of these operations required is the numerical measure of the distance AB. This is the basis of all measurement of length. 1 Every description of the scene of an event or of the position of an object in space is based on the specification of the point on a rigid body (body of reference) with which that event or object coincides. This applies not only to scientific description, but also to everyday life. If I analyse the place specification “Trafalgar Square, London,” 1 I arrive at the following result. The earth is the rigid body to which the specification of place refers; “Trafalgar Square, London” is a welldefined point, to which a name has been assigned, and with which the event coincides in space. 2 This primitive method of place specification deals only with places on the surface of rigid bodies, and is dependent on the existence of points on this surface which are distinguishable from each other. But we can free ourselves from both of these limitations without altering the nature of our specification of position. If, for instance, a cloud is hovering over Trafalgar Square, then we can determine its position relative to the surface of the earth by erecting a pole perpendicularly on the Square, so that it reaches the cloud. The length of the pole measured with the standard measuring-rod, combined with the specification of the position of the foot of the pole, supplies us with a complete place specification. On the basis of this illustration, we are able to see the manner in which a refinement of the conception of position has been developed. (a) We imagine the rigid body, to which the place specification is referred, supplemented in such a manner that the object whose position we require is reached by the completed rigid body. (b) In locating the position of the object, we make use of a number (here the length of the pole measured with the measuring-rod) instead of designated points of reference. (c) We speak of the height of the cloud even when the pole which reaches the cloud has not been erected. By means of optical observations of the cloud from different positions on the ground, and taking into account the properties of the propagation of light, we determine the length of the pole we should have required in order to reach the cloud. From this consideration we see that it will be advantageous if, in the description of position, it should be possible by means of numerical measures to make ourselves independent of the existence of marked positions (possessing names) on the rigid body of reference. In the physics of measurement this is attained by the application of the Cartesian system of co-ordinates. This consists of three plane surfaces perpendicular to each other and rigidly attached to a rigid body. Referred to a system of co-ordinates, the scene of any event will be determined (for the main part) by the specification of the lengths of the three perpendiculars or co-ordinates (x, y, z) which can be dropped from the scene of the event to those three plane surfaces. The lengths of these three perpendiculars can be determined by a series of manipulations with rigid measuringrods performed according to the rules and methods laid down by Euclidean geometry. In practice, the rigid surfaces which constitute the system of co-ordinates are generally not available; furthermore, the magnitudes of the coordinates are not actually determined by constructions with rigid rods, but by indirect means. If the results of physics and astronomy are to maintain their clearness, the physical meaning of specifications of position must always be sought in accordance with the above considerations. 1 We thus obtain the following result: Every description of events in space involves the use of a rigid body to which such events have to be referred. The resulting relationship takes for granted that the laws of Euclidean geometry hold for “distances,” the “distance” being represented physically by means of the convention of two marks on a rigid body. The purpose of mechanics is to describe how bodies change their position in space with time.” I should load my conscience with grave sins against the sacred spirit of lucidity were I to formulate the aims of mechanics in this way, without serious reflection and detailed explanations. Let us proceed to disclose these sins. It is not clear what is to be understood here by “position” and “space.” I stand at the window of a railway carriage which is travelling uniformly, and drop a stone on the embankment, without throwing it. Then, disregarding the influence of the air resistance, I see the stone descend in a straight line. A pedestrian who observes the misdeed from the footpath notices that the stone falls to earth in a parabolic curve. I now ask: Do the “positions” traversed by the stone lie “in reality” on a straight line or on a parabola? Moreover, what is meant here by motion “in space”? From the considerations of the previous section the answer is self-evident. In the first place, we entirely shun the vague word “space,” of which, we must honestly acknowledge, we cannot form the slightest conception, and we replace it by “motion relative to a practically rigid body of reference.” The positions relative to the body of reference (railway carriage or embankment) have already been defined in detail in the preceding section. If instead of “body of reference” we insert “system of co-ordinates,” which is a useful idea for mathematical description, we are in a position to say: The stone traverses a straight line relative to a system of co-ordinates rigidly attached to the carriage, but relative to a system of co-ordinates rigidly attached to the ground (embankment) it describes a parabola. With the aid of this example it is clearly seen that there is no such thing as an independently existing trajectory (lit. “path-curve” 1 ), but only a trajectory relative to a particular body of reference. In order to have a complete description of the motion, we must specify how the body alters its position with time; i.e. for every point on the trajectory it must be stated at what time the body is situated there. These data must be supplemented by such a definition of time that, in virtue of this definition, these time-values can be regarded essentially as magnitudes (results of measurements) capable of observation. If we take our stand on the ground of classical mechanics, we can satisfy this requirement for our illustration in the following manner. We imagine two clocks of identical construction; the man at the railway-carriage window is holding one of them, and the man on the footpath the other. Each of the observers determines the position on his own reference-body occupied by the stone at each tick of the clock he is holding in his hand. In this connection we have not taken account of the inaccuracy involved by the finiteness of the velocity of propagation of light. With this and with a second difficulty prevailing here we shall have to deal in detail later. As is well known, the fundamental law of the mechanics of Galilei-Newton, which is known as the law of inertia, can be stated thus: A body removed sufficiently far from other bodies continues in a state of rest or of uniform motion in a straight line. This law not only says something about the motion of the bodies, but it also indicates the reference-bodies or systems of co-ordinates, permissible in mechanics, which can be used in mechanical description. The visible fixed stars are bodies for which the law of inertia certainly holds to a high degree of approximation. Now if we use a system of co-ordinates which is rigidly attached to the earth, then, relative to this system, every fixed star describes a circle of immense radius in the course of an astronomical day, a result which is opposed to the statement of the law of inertia. So that if we adhere to this law we must refer these motions only to systems of coordinates relative to which the fixed stars do not move in a circle. A system of co-ordinates of which the state of motion is such that the law of inertia holds relative to it is called a “Galileian system of co-ordinates.” The laws of the mechanics of Galilei-Newton can be regarded as valid only for a Galileian system of co-ordinates. In order to attain the greatest possible clear- ness, let us return to our example of the rail- way carriage supposed to be travelling uniformly. We call its motion a uniform translation (“uniform” because it is of constant velocity and direction, “translation” because although the carriage changes its position relative to the embankment yet it does not rotate in so doing). Let us imagine a raven flying through the air in such a manner that its motion, as observed from the embankment, is uniform and in a straight line. If we were to observe the flying raven from the moving railway carriage, we should find that the motion of the raven would be one of different velocity and direction, but that it would still be uniform and in a straight line. Expressed in an abstract manner we may say: If a mass m is moving uniformly in a straight line with respect to a co-ordinate system K, then it will also be moving uniformly and in a straight line relative to a second co-ordinate system K', provided that the latter is executing a uniform translatory motion with respect to K.
The Hubble Deep Field (HDF) is an image of a small region in the constellation Ursa Major, constructed from a series of observations by the Hubble Space Telescope. It covers an area about 2.6 arcminutes on a side, about one 24-millionth of the whole sky, which is equivalent in angular size to a tennis ball at a distance of 100 metres.[1] The image was assembled from 342 separate exposures taken with the Space Telescope's Wide Field and Planetary Camera 2 over ten consecutive days between December 18 and 28, 1995.[2][3]The field is so small that only a few foreground stars in the Milky Way lie within it; thus, almost all of the 3,000 objects in the image are galaxies, some of which are among the youngest and most distant known. By revealing such large numbers of very young galaxies, the HDF has become a landmark image in the study of the early universe.Three years after the HDF observations were taken, a region in the south celestial hemisphere was imaged in a similar way and named the Hubble Deep Field South. The similarities between the two regions strengthened the belief that the universe is uniform over large scales and that the Earth occupies a typical region in the Universe (the cosmological principle). A wider but shallower survey was also made as part of the Great Observatories Origins Deep Survey. In 2004 a deeper image, known as the Hubble Ultra-Deep Field (HUDF), was constructed from a few months of light exposure. The HUDF image was at the time the most sensitive astronomical image ever made at visible wavelengths, and it remained so until the Hubble eXtreme Deep Field (XDF) was released in 2012. One of the key aims of the astronomers who designed the Hubble Space Telescope was to use its high optical resolution to study distant galaxies to a level of detail that was not possible from the ground. Positioned above the atmosphere, Hubble avoids atmospheric airglow allowing it to take more sensitive visible and ultraviolet light images than can be obtained with seeing-limited ground-based telescopes (when good adaptive optics correction at visible wavelengths becomes possible, 10 m ground-based telescopes may become competitive). Although the telescope's mirror suffered from spherical aberration when the telescope was launched in 1990, it could still be used to take images of more distant galaxies than had previously been obtainable. Because light takes billions of years to reach Earth from very distant galaxies, we see them as they were billions of years ago; thus, extending the scope of such research to increasingly distant galaxies allows a better understanding of how they evolve.[2]After the spherical aberration was corrected during Space Shuttle mission STS-61 in 1993,[4] the improved imaging capabilities of the telescope were used to study increasingly distant and faint galaxies. The Medium Deep Survey (MDS) used the Wide Field and Planetary Camera 2 (WFPC2) to take deep images of random fields while other instruments were being used for scheduled observations. At the same time, other dedicated programs focused on galaxies that were already known through ground-based observation. All of these studies revealed substantial differences between the properties of galaxies today and those that existed several billion years ago.[5]Up to 10% of the HST's observation time is designated as Director's Discretionary (DD) Time, and is typically awarded to astronomers who wish to study unexpected transient phenomena, such as supernovae. Once Hubble's corrective optics were shown to be performing well, Robert Williams, the then-director of the Space Telescope Science Institute, decided to devote a substantial fraction of his DD time during 1995 to the study of distant galaxies. A special Institute Advisory Committee recommended that the WFPC2 be used to image a "typical" patch of sky at a high galactic latitude, using several optical filters. A working group was set up to develop and implement the project.[6] The field selected for the observations needed to fulfill several criteria. It had to be at a high galactic latitude, because dust and obscuring matter in the plane of the Milky Way's disc prevents observations of distant galaxies at low galactic latitudes. The target field had to avoid known bright sources of visible light (such as foreground stars), and infrared, ultraviolet and X-ray emissions, to facilitate later studies at many wavelengths of the objects in the deep field, and also needed to be in a region with a low background infrared 'cirrus', the diffuse, wispy infrared emission believed to be caused by warm dust grains in cool clouds of hydrogen gas (H I regions).[6]These criteria restricted the field of potential target areas. It was decided that the target should be in Hubble's 'continuous viewing zones' (CVZs)—the areas of sky which are not occulted by the Earth or the moon during Hubble's orbit.[6] The working group decided to concentrate on the northern CVZ, so that northern-hemisphere telescopes such as the Keck telescopes, the Kitt Peak National Observatory telescopes and the Very Large Array (VLA) could conduct follow-up observations.[7]Twenty fields satisfying these criteria were initially identified, from which three optimal candidate fields were selected, all within the constellation of Ursa Major. Radio snapshot observations with the VLA ruled out one of these fields because it contained a bright radio source, and the final decision between the other two was made on the basis of the availability of guide stars near the field: Hubble observations normally require a pair of nearby stars on which the telescope's Fine Guidance Sensors can lock during an exposure, but given the importance of the HDF observations, the working group required a second set of back-up guide stars. The field that was eventually selected is located at a right ascension of 12h 36m 49.4s and a declination of +62° 12′ 58″;[6][7] it is approximately 2.6 arcminutes in width,[2][8] or 1/12 the width of the Moon. The area is approximately 1/24,000,000 of the total area of the sky. Once a field had been selected, an observing strategy had to be developed. An important decision was to determine which filters the observations would use; WFPC2 is equipped with forty-eight filters, including narrowband filters isolating particular emission lines of astrophysical interest, and broadband filters useful for the study of the colours of stars and galaxies. The choice of filters to be used for the HDF depended on the 'throughput' of each filter—the total proportion of light that it allows through—and the spectral coverage available. Filters with bandpasses overlapping as little as possible were desirable.[6]In the end, four broadband filters were chosen, centred at wavelengths of 300 nm (near-ultraviolet), 450 nm (blue light), 606 nm (red light) and 814 nm (near-infrared). Because the quantum efficiency of Hubble's detectors at 300 nm wavelength is quite low, the noise in observations at this wavelength is primarily due to CCD noise rather than sky background; thus, these observations could be conducted at times when high background noise would have harmed the efficiency of observations in other passbands.[6]Between December 18 and 28, 1995—during which time Hubble orbited the Earth about 150 times—342 images of the target area in the chosen filters were taken. The total exposure times at each wavelength were 42.7 hours (300 nm), 33.5 hours (450 nm), 30.3 hours (606 nm) and 34.3 hours (814 nm), divided into 342 individual exposures to prevent significant damage to individual images by cosmic rays, which cause bright streaks to appear when they strike CCD detectors. A further 10 Hubble orbits were used to make short exposures of flanking fields to aid follow-up observations by other instruments.[6] The production of a final combined image at each wavelength was a complex process. Bright pixels caused by cosmic ray impacts during exposures were removed by comparing exposures of equal length taken one after the other, and identifying pixels that were affected by cosmic rays in one exposure but not the other. Trails of space debris and artificial satellites were present in the original images, and were carefully removed.[6]Scattered light from the Earth was evident in about a quarter of the data frames, creating a visible "X" pattern on the images. This was removed by taking an image affected by scattered light, aligning it with an unaffected image, and subtracting the unaffected image from the affected one. The resulting image was smoothed, and could then be subtracted from the bright frame. This procedure removed almost all of the scattered light from the affected images.[6]Once the 342 individual images were cleaned of cosmic-ray hits and corrected for scattered light, they had to be combined. Scientists involved in the HDF observations pioneered a technique called 'drizzling', in which the pointing of the telescope was varied minutely between sets of exposures. Each pixel on the WFPC2 CCD chips recorded an area of sky 0.09 arcseconds across, but by changing the direction in which the telescope was pointing by less than that between exposures, the resulting images were combined using sophisticated image-processing techniques to yield a final angular resolution better than this value. The HDF images produced at each wavelength had final pixel sizes of 0.03985 arcseconds.[6]The data processing yielded four monochrome images (at 300 nm, 450 nm, 606 nm and 814 nm), one at each wavelength.[9] One image was designated as red (814 nm), the second as green (606 nm) and the third as blue (450 nm), and the three images were combined to give a colour image.[3] Because the wavelengths at which the images were taken do not correspond to the wavelengths of red, green and blue light, the colours in the final image only give an approximate representation of the actual colours of the galaxies in the image; the choice of filters for the HDF (and the majority of Hubble images) was primarily designed to maximize the scientific utility of the observations rather than to create colours corresponding to what the human eye would actually perceive.[9] The final images were released at a meeting of the American Astronomical Society in January 1996,[10] and revealed a plethora of distant, faint galaxies. About 3,000 distinct galaxies could be identified in the images,[11] with both irregular and spiral galaxies clearly visible, although some galaxies in the field are only a few pixels across. In all, the HDF is thought to contain fewer than twenty galactic foreground stars; by far the majority of objects in the field are distant galaxies.[12]There are about fifty blue point-like objects in the HDF. Many seem to be associated with nearby galaxies, which together form chains and arcs: these are likely to be regions of intense star formation. Others may be distant quasars. Astronomers initially ruled out the possibility that some of the point-like objects are white dwarfs, because they are too blue to be consistent with theories of white dwarf evolution prevalent at the time. However, more recent work has found that many white dwarfs become bluer as they age, lending support to the idea that the HDF might contain white dwarfs.[13] The HDF data provided extremely rich material for cosmologists to analyse and by late 2014 the associated scientific paper for the image had received over 900 citations.[15] One of the most fundamental findings was the discovery of large numbers of galaxies with high redshift values.As the Universe expands, more distant objects recede from the Earth faster, in what is called the Hubble Flow. The light from very distant galaxies is significantly affected by the cosmological redshift. While quasars with high redshifts were known, very few galaxies with redshifts greater than one were known before the HDF images were produced.[10] The HDF, however, contained many galaxies with redshifts as high as six, corresponding to distances of about 12 billion light-years. Due to redshift the most distant objects in the HDF (Lyman-break galaxies) are not actually visible in the Hubble images; they can only be detected in images of the HDF taken at longer wavelengths by ground-based telescopes.[16]The HDF galaxies contained a considerably larger proportion of disturbed and irregular galaxies than the local universe;[10] galaxy collisions and mergers were more common in the young universe as it was much smaller than today. It is believed that giant elliptical galaxies form when spirals and irregular galaxies collide.
Newton's law of universal gravitation is usually stated as that every particle attracts every other particle in the universe with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between their centers.[note 1] The publication of the theory has become known as the "first great unification", as it marked the unification of the previously described phenomena of gravity on Earth with known astronomical behaviors.[1][2][3]This is a general physical law derived from empirical observations by what Isaac Newton called inductive reasoning.[4] It is a part of classical mechanics and was formulated in Newton's work Philosophiæ Naturalis Principia Mathematica ("the Principia"), first published on 5 July 1687. When Newton presented Book 1 of the unpublished text in April 1686 to the Royal Society, Robert Hooke made a claim that Newton had obtained the inverse square law from him.In today's language, the law states that every point mass attracts every other point mass by a force acting along the line intersecting the two points. The force is proportional to the product of the two masses, and inversely proportional to the square of the distance between them.[5]where F is the gravitational force acting between two objects, m1 and m2 are the masses of the objects, r is the distance between the centers of their masses, and G is the gravitational constant.The first test of Newton's theory of gravitation between masses in the laboratory was the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798.[6] It took place 111 years after the publication of Newton's Principia and approximately 71 years after his death.Newton's law of gravitation resembles Coulomb's law of electrical forces, which is used to calculate the magnitude of the electrical force arising between two charged bodies. Both are inverse-square laws, where force is inversely proportional to the square of the distance between the bodies. Coulomb's law has the product of two charges in place of the product of the masses, and the Coulomb constant in place of the gravitational constant.Newton's law has since been superseded by Albert Einstein's theory of general relativity, but it continues to be used as an excellent approximation of the effects of gravity in most applications. Relativity is required only when there is a need for extreme accuracy, or when dealing with very strong gravitational fields, such as those found near extremely massive and dense objects, or at small distances (such as Mercury's orbit around the Sun).The relation of the distance of objects in free fall to the square of the time taken had recently been confirmed by Grimaldi and Riccioli between 1640 and 1650. They had also made a calculation of the gravity of Earth by recording the oscillations of a pendulum.[7]A modern assessment about the early history of the inverse square law is that "by the late 1670s", the assumption of an "inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons".[8] The same author credits Robert Hooke with a significant and seminal contribution, but treats Hooke's claim of priority on the inverse square point as irrelevant, as several individuals besides Newton and Hooke had suggested it. He points instead to the idea of "compounding the celestial motions" and the conversion of Newton's thinking away from "centrifugal" and towards "centripetal" force as Hooke's significant contributions.Newton gave credit in his Principia to two people: Bullialdus (who wrote without proof that there was a force on the Earth towards the Sun), and Borelli (who wrote that all planets were attracted towards the Sun).[9][10] The main influence may have been Borelli, whose book Newton had a copy of.[11]In 1686, when the first book of Newton's Principia was presented to the Royal Society, Robert Hooke accused Newton of plagiarism by claiming that he had taken from him the "notion" of "the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that "the Demonstration of the Curves generated thereby" was wholly Newton's.[12]Robert Hooke published his ideas about the "System of the World" in the 1660s, when he read to the Royal Society on March 21, 1666, a paper "concerning the inflection of a direct motion into a curve by a supervening attractive principle", and he published them again in somewhat developed form in 1674, as an addition to "An Attempt to Prove the Motion of the Earth from Observations".[13] Hooke announced in 1674 that he planned to "explain a System of the World differing in many particulars from any yet known", based on three suppositions: that "all Celestial Bodies whatsoever, have an attraction or gravitating power towards their own Centers" and "also attract all the other Celestial Bodies that are within the sphere of their activity";[14] that "all bodies whatsoever that are put into a direct and simple motion, will so continue to move forward in a straight line, till they are by some other effectual powers deflected and bent..." and that "these attractive powers are so much the more powerful in operating, by how much the nearer the body wrought upon is to their own Centers". Thus Hooke postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, together with a principle of linear inertia.Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses.[15] He also did not provide accompanying evidence or mathematical demonstration. On the latter two aspects, Hooke himself stated in 1674: "Now what these several degrees [of attraction] are I have not yet experimentally verified"; and as to his whole proposal: "This I only hint at present", "having my self many other things in hand which I would first compleat, and therefore cannot so well attend it" (i.e. "prosecuting this Inquiry").[13] It was later on, in writing on 6 January 1679|80[16] to Newton, that Hooke communicated his "supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance."[17] (The inference about the velocity was incorrect.)[18]Hooke's correspondence with Newton during 1679–1680 not only mentioned this inverse square supposition for the decline of attraction with increasing distance, but also, in Hooke's opening letter to Newton, of 24 November 1679, an approach of "compounding the celestial motions of the planets of a direct motion by the tangent & an attractive motion towards the central body".[19]Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea. Among the reasons, Newton recalled that the idea had been discussed with Sir Christopher Wren previous to Hooke's 1679 letter.[20] Newton also pointed out and acknowledged prior work of others,[21] including Bullialdus,[9] (who suggested, but without demonstration, that there was an attractive force from the Sun in the inverse square proportion to the distance), and Borelli[10] (who suggested, also without demonstration, that there was a centrifugal tendency in counterbalance with a gravitational attraction towards the Sun so as to make the planets move in ellipses). D T Whiteside has described the contribution to Newton's thinking that came from Borelli's book, a copy of which was in Newton's library at his death.[11]Newton further defended his work by saying that had he first heard of the inverse square proportion from Hooke, he would still have some rights to it in view of his demonstrations of its accuracy. Hooke, without evidence in favor of the supposition, could only guess that the inverse square law was approximately valid at great distances from the center. According to Newton, while the 'Principia' was still at pre-publication stage, there were so many a priori reasons to doubt the accuracy of the inverse-square law (especially close to an attracting sphere) that "without my (Newton's) Demonstrations, to which Mr Hooke is yet a stranger, it cannot believed by a judicious Philosopher to be any where accurate."[22]This remark refers among other things to Newton's finding, supported by mathematical demonstration, that if the inverse square law applies to tiny particles, then even a large spherically symmetrical mass also attracts masses external to its surface, even close up, exactly as if all its own mass were concentrated at its center. Thus Newton gave a justification, otherwise lacking, for applying the inverse square law to large spherical planetary masses as if they were tiny particles.[23] In addition, Newton had formulated, in Propositions 43–45 of Book 1[24] and associated sections of Book 3, a sensitive test of the accuracy of the inverse square law, in which he showed that only where the law of force is calculated as the inverse square of the distance will the directions of orientation of the planets' orbital ellipses stay constant as they are observed to do apart from small effects attributable to inter-planetary perturbations. In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, "endeavour to recede" (what was later called centrifugal force) had an inverse-square relation with distance from the center.[25] After his 1679–1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis.[26] This background shows there was basis for Newton to deny deriving the inverse square law from Hooke. On the other hand, Newton did accept and acknowledge, in all editions of the Principia, that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke, and Halley in this connection in the Scholium to Proposition 4 in Book 1.[27] Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: "yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ..."[21]Since the time of Newton and Hooke, scholarly discussion has also touched on the question of whether Hooke's 1679 mention of 'compounding the motions' provided Newton with something new and valuable, even though that was not a claim actually voiced by Hooke at the time. As described above, Newton's manuscripts of the 1660s do show him actually combining tangential motion with the effects of radially directed force or endeavour, for example in his derivation of the inverse square relation for the circular case. They also show Newton clearly expressing the concept of linear inertia—for which he was indebted to Descartes' work, published in 1644 (as Hooke probably was).[28] These matters do not appear to have been learned by Newton from Hooke.Nevertheless, a number of authors have had more to say about what Newton gained from Hooke and some aspects remain controversial.[8] The fact that most of Hooke's private papers had been destroyed or have disappeared does not help to establish the truth. Newton's role in relation to the inverse square law was not as it has sometimes been represented. He did not claim to think it up as a bare idea. What Newton did, was to show how the inverse-square law of attraction had many necessary mathematical connections with observable features of the motions of bodies in the solar system; and that they were related in such a way that the observational evidence and the mathematical demonstrations, taken together, gave reason to believe that the inverse square law was not just approximately true but exactly true (to the accuracy achievable in Newton's time and for about two centuries afterwards – and with some loose ends of points that could not yet be certainly examined, where the implications of the theory had not yet been adequately identified or calculated).[29][30]
A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.[1][2][3] This makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6]The term “recurrent neural network” is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is finite impulse and the other is infinite impulse. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph, if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).In typical libraries like PyTorch Just-in-time compilation plays an important role for efficiently implementing recurrent neural networks.Recurrent neural networks were based on David Rumelhart's work in 1986.[8] Hopfield networks – a special kind of RNN – were discovered by John Hopfield in 1982. In 1993, a neural history compressor system solved a “Very Deep Learning” task that required more than 1000 subsequent layers in an RNN unfolded in time.[9]Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.[10]Around 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.[11] In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition.[12][13] In 2014, the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset[14] benchmark without using any traditional speech processing methods.[15]LSTM also improved large-vocabulary speech recognition[5][6] and text-to-speech synthesis[16] and was used in Google Android.[12][17] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49%[citation needed] through CTC-trained LSTM.[18]LSTM broke records for improved machine translation,[19] Language Modeling[20] and Multilingual Language Processing.[21] LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.[22]Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in "layers" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'. It is "unfolded" in time to produce the appearance of layers. An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one.[23] At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also referred to as the state layer. They have a recurrent connection to themselves.[23]Elman and Jordan networks are also known as “Simple recurrent networks” (SRN).The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration. Introduced by Bart Kosko,[26] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.[27]A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.[28]The echo state network (ESN) has a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series.[29] A variant for spiking neurons is known as a liquid state machine.[30]The Independently recurrent neural network (IndRNN)[31] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with the non-saturated nonlinear functions such as ReLU. Using skip connections, deep networks can be trained. A recursive neural network[32] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation.[33][34] They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing.[35] The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.[36]The neural history compressor is an unsupervised stack of RNNs.[37] At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.The system effectively minimises the description length or the negative logarithm of the probability of the data.[38] Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.It is possible to distill the RNN hierarchy into two RNNs: the "conscious" chunker (higher level) and the "subconscious" automatizer (lower level).[37] Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.[37]A generative model partially overcame the vanishing gradient problem[39] of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a “Very Deep Learning” task that required more than 1000 subsequent layers in an RNN unfolded in time.[9]Second order RNNs use higher order weights  instead of the standard  weights, and states can be a product. This allows a direct mapping to a finite state machine both in training, stability, and representation.[40][41] Long short-term memory is an example of this but has no such formal mappings or proof of stability.Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called “forget gates”.[42] LSTM prevents backpropagated errors from vanishing or exploding.[39] Instead, errors can flow backwards through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks[12] that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved.[43] LSTM works even given long delays between significant events and can handle signals that mix low and high frequency components.Many applications use stacks of LSTM RNNs[44] and train them by Connectionist Temporal Classification (CTC)[45] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.[46]Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants.[47][48] Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.[49] They have fewer parameters than LSTM, as they lack an output gate.[50] Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs.[51][52]A continuous time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train.Hierarchical RNNs connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.[37][57] Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.[58]Generally, a recurrent multilayer perceptron network (RMLP) network consists of cascaded subnetworks, each of which contains multiple layers of nodes. Each of these subnetworks is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed forward connections.[59]A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.[60][61] With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.[citation needed] Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.[62][63]Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices.[66] The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures which may be based on memristive systems. Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have a continuous dynamics, have a limited memory capacity and they natural relax via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit has the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering an analog memristive networks accounts to a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring, or topology. [67][68]
Psychology is the science of mind and behavior. Psychology includes the study of conscious and unconscious phenomena, as well as feelings and thought. It is an academic discipline of immense scope. Psychologists also seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As a social science, psychologists aim to understand the behavior of individuals and groups.[1][2]A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as a social, behavioral, or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and biological processes that underlie cognitive functions and behaviors.Psychologists explore behavior and mental processes, including perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. Psychologists also consider the unconscious mind.[3] Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation.While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society.[4][5] Many psychologists are involved in some kind of therapeutic role, practicing in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, hospitals). Another group of psychologists is employed in industrial and organizational settings.[6] Yet others are involved in work on human development, aging, sports, health, forensics, and the media.he word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word "psychology" derives from -λογία -logia, which refers to "study" or "research".[7] The Latin word psychologia was first used by the Croatian humanist and Latinist Marko Marulić in his book, Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the late 15th century or early 16th century.[8] The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to "Anatomy, which treats the Body, and Psychology, which treats of the Soul."[9]In 1890, William James defined psychology as "the science of mental life, both of its phenomena and their conditions."[10] This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by radical behaviorists such as John B. Watson, who in 1913 defined the discipline as a "natural science," the theoretical goal of which "is the prediction and control of behavior."[11] Since James defined "psychology," the term more strongly implicates scientific experimentation.[12][11] Folk psychology refers to the understanding of ordinary people, as contrasted with that of psychology professionals.[13] The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders.[14] Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise),[15] addressed the workings of the mind.[16] As early as the 4th century BC, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.[17]In China, psychological understanding grew from the philosophical works of Laozi and Confucius, and later from the doctrines of Buddhism. This body of knowledge involves insights drawn from introspection and observation, as well as techniques for focused thinking and acting. It frames the universe in term of a division of physical reality and mental reality as well as the interaction between the physical and the mental. Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing Dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.[18]Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India.[19][20]Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant advanced the idea of anthropology as a discipline, with psychology an important subdivision. Kant, however, explicitly rejected the idea of an experimental psychology, writing that "the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object." In 1783, Ferdinand Ueberwasser (1752-1812) designated himself Professor of Empirical Psychology and Logic and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars.[21] At the end of the Napoleonic era, Prussian authorities discontinued the Old University of Münster.[22] Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation.[23] In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's crowded "lunatic" asylums.Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s. He articulated the principle that human perception of a stimulus varies logarithmically according to its intensity.[25] The principle became known as the Weber–Fechner law. Fechner's 1860 Elements of Psychophysics challenged Kant's negative view with regard to conducting quantitative research on the mind.[26][23] Fechner's achievement was to show that "mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods."[23] In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, where he established the psychological laboratory that brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of materials.[27] Paul Flechsig and Emil Kraepelin soon created another influential laboratory at Leipzig, a psychology-related lab, that focused more on experimental psychiatry.[23]The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was another 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting.[28] In the early twentieth century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology (not to be confused with the Gestalt therapy of Fritz Perls). The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintained that whole of experience is important, and differs from the sum of its parts.Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories.[29] G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally influential. The lab was located at Johns Hopkins University. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo.[30] Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta.[19] Wundt's students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests of mental ability. Cattell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Witmer focused on the mental testing of children; Scott, on employee selection.[31]Another student of Wundt, the Englishman Edward Titchener, created the psychology program at Cornell University and advanced "structuralist" psychology. The idea behind structuralism was to analyze and classify different aspects of the mind, primarily through the method of introspection.[32] William James, John Dewey, and Harvey Carr advanced the idea of functionalism, an expansive approach to psychology that underlined the Darwinian idea of a behavior's usefulness to the individual. In 1890, James wrote an influential book, The Principles of Psychology, which expanded on the structuralism. He memorably described "stream of consciousness." James's ideas interested many American students in the emerging discipline.[32][33][34] Dewey integrated psychology with societal concerns, most notably by promoting progressive education, inculcating moral values in children, and assimilating immigrants.[35]A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires.[36] In Russia, too, researchers placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, "Who Is to Develop Psychology and How?" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic view of human behavior.[37] The Russian-Soviet physiologist Ivan Pavlov discovered in dogs a learning process that was later termed "classical conditioning" and applied the process to human beings.[38]One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted from 1885 to 1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the four hundred attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held at different locations in Europe and with wide international participation. The Sixth Congress, held in Geneva in 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA.[29] Tokyo Imperial University led the way in bringing new psychology to the East. New ideas about psychology diffused from Japan into China.[18][30]American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million soldiers.[39] Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research.[40][41] Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing.[39][42] Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S.[43] Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes.[44] In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England.
Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.[1][2][3]Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.Sub-domains of computer vision include scene reconstruction, object detection, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[1][2][3] "Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding."[9] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[10] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.In the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it "describe what it saw".[12][13]What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11]The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[14] By the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11]Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.[15][16] The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.[citation needed]Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infrared light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.A third field which plays an important role is neurobiology, specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how "real" vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in biology.Some strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[17]Yet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision that have no counterpart in processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[18] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.Beside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion ecommerce, inventory management, patent search, furniture, and the beauty industry.The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.Computer graphics produces image data from 3D models, computer vision often produces 3D models from image data.[19] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is detection of tumours, arteriosclerosis or other malign changes; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain, or about the quality of medical treatments. Applications of computer vision in the medical area also includes enhancement of images interpreted by humans—ultrasonic images or X-ray images for example—to reduce the influence of noise.A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.[24] Military applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as "battlefield awareness", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.